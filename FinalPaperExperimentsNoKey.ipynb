{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "891dc1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40aab47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "import fitz\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from prompts3 import *\n",
    "import pandas as pd\n",
    "import random\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad460d4f",
   "metadata": {},
   "source": [
    "### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6821a8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HumanRubric(rubric_file):\n",
    "    \"\"\"Create data frame of human scoring rubric given csv file of scored rubric. \n",
    "       Index represents question number\n",
    "\n",
    "    Arguments:\n",
    "        file - filepath name of human scored csv file\n",
    "    Returns:\n",
    "        human_rubric_df - df of human scored rubric  \n",
    "    \"\"\"\n",
    "    # Create cleaned DF of questions \n",
    "    human_rubric_df = pd.read_csv(rubric_file)\n",
    "    human_rubric_df = human_rubric_df.iloc[10:, 3:]\n",
    "    human_rubric_df = human_rubric_df.reset_index()\n",
    "    human_rubric_df = human_rubric_df.drop('index', axis=1)\n",
    "    human_rubric_df.columns = human_rubric_df.iloc[0]\n",
    "    human_rubric_df = human_rubric_df[1:]\n",
    "\n",
    "    return human_rubric_df\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file, preserving page breaks.\n",
    "\n",
    "    :param pdf_path: The path to the PDF file.\n",
    "    :return: A string containing the extracted text with page breaks.\n",
    "    \"\"\"\n",
    "    document = fitz.open(pdf_path)\n",
    "    text = []\n",
    "\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        text.append(page.get_text(\"text\"))\n",
    "    \n",
    "    # Join the text of each page with form feed characters to indicate page breaks\n",
    "    return '\\f'.join(text)\n",
    "\n",
    "def split_into_pages(text):\n",
    "    \"\"\"\n",
    "    Splits the text into pages.\n",
    "    \n",
    "    :param text: The complete text extracted from the PDF.\n",
    "    :return: A list of pages.\n",
    "    \"\"\"\n",
    "    pages = text.split('\\f')\n",
    "    return pages\n",
    "\n",
    "def get_embedding(text):\n",
    "    \"\"\"\n",
    "    Fetches the embedding for a given text using OpenAI's API.\n",
    "    \n",
    "    :param text: The text to get the embedding for.\n",
    "    :return: A vector representation of the text.\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=key)\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input = [text], model=\"text-embedding-3-large\").data[0].embedding\n",
    "\n",
    "def get_embeddings(full_text):\n",
    "    \"\"\"\n",
    "    Splits the full text into pages and computes embeddings for each page.\n",
    "\n",
    "    :param full_text: The complete text extracted from the source.\n",
    "    :return: A list of text pages and their corresponding embeddings.\n",
    "    \"\"\"\n",
    "    pages = split_into_pages(full_text)\n",
    "    embeddings = []\n",
    "    for page in pages:\n",
    "        page_embedding = get_embedding(page)\n",
    "        embeddings.append(page_embedding)\n",
    "    return pages, embeddings\n",
    "\n",
    "def find_most_relevant_pages(pages, embeddings, question, top_n=10):\n",
    "    \"\"\"\n",
    "    Finds the most relevant text pages to a given question using pre-computed embeddings and cosine similarity.\n",
    "\n",
    "    :param pages: A list of text pages corresponding to the embeddings.\n",
    "    :param embeddings: A list of embeddings for each page of text.\n",
    "    :param question: The question for which to find the relevant pages.\n",
    "    :param top_n: The number of top relevant pages to return.\n",
    "    :return: The top N most relevant pages of the text.\n",
    "    \"\"\"\n",
    "    question_embedding = get_embedding(question) \n",
    "    scores = []\n",
    "\n",
    "    # Iterate over the embeddings and pages together\n",
    "    for page, page_embedding in zip(pages, embeddings):\n",
    "        score = cosine_similarity([question_embedding], [page_embedding])[0][0]\n",
    "        scores.append((score, page))\n",
    "        \n",
    "    # Sort the pages by score in descending order\n",
    "    scores.sort(reverse=True, key=lambda x: x[0])\n",
    "    \n",
    "    # Get the top N pages\n",
    "    most_relevant_pages = [page for score, page in scores[:top_n]]\n",
    "    \n",
    "    # Combine the most relevant pages into a continuous string\n",
    "    continuous_text = '\\n'.join(most_relevant_pages)\n",
    "    \n",
    "    return continuous_text\n",
    "\n",
    "def compare_lists(list1, list2):\n",
    "    differences = []\n",
    "    if len(list1) != len(list2):\n",
    "        return None, \"Lists are of different lengths and cannot be compared index by index.\"\n",
    "    for i in range(len(list1)):\n",
    "        if list1[i] != list2[i]:\n",
    "            differences.append(i)\n",
    "    return differences\n",
    "\n",
    "def ChatGDE(pages, embeddings, prompts, rubric_file, no_test, gde_answer_function):\n",
    "    answers = []\n",
    "    \n",
    "    # Generate answers based on pages and embeddings\n",
    "    for i in range(1, 71):\n",
    "        if i not in no_test:\n",
    "            section = find_most_relevant_pages(pages, embeddings, prompts[i-1])\n",
    "            section_and_question = section + prompts[i-1]\n",
    "            answers.append(gde_answer_function(section_and_question))\n",
    "    \n",
    "    # Load rubric and process chat answers\n",
    "    rubric = HumanRubric(rubric_file)\n",
    "    chat_answers = rubric['Answer']\n",
    "    chat_answers = chat_answers.drop(no_test, errors='ignore')\n",
    "    chat_answers = ['No' if item == 'Somewhat' else item for item in chat_answers]\n",
    "    chat_answers = ['NotApplicable' if item == 'Not Applicable' else item for item in chat_answers]\n",
    "    \n",
    "    # Compare lists and print differences\n",
    "    differences = compare_lists(answers, chat_answers)\n",
    "    if differences:\n",
    "        print(\"Differences at indices:\", differences)\n",
    "    else:\n",
    "        print(\"No differences. Lists are identical.\")\n",
    "\n",
    "    # Print count of differences\n",
    "    print(f\"Number of differences: {len(differences)}\")\n",
    "\n",
    "def extract_yes_probabilities(responses: list) -> list:\n",
    "    # Define a mapping of confidence descriptions to their corresponding probabilities\n",
    "    confidence_mapping = {\n",
    "        \"100%\": 1.0,\n",
    "        \"85%\": 0.85,\n",
    "        \"75%\": 0.75,\n",
    "        \"60%\": 0.60,\n",
    "        \"50%\": 0.50\n",
    "    }\n",
    "    \n",
    "    yes_probabilities = []\n",
    "    \n",
    "    for response in responses:\n",
    "        parts = response.split(', ')\n",
    "        answer = parts[0]\n",
    "        confidence_percentage = parts[-1]  # Extract the last part, which should be the percentage\n",
    "        probability = confidence_mapping.get(confidence_percentage, 0)  # Default to 0 if not found\n",
    "        \n",
    "        if answer == \"Yes\":\n",
    "            yes_probabilities.append(probability)\n",
    "        else:\n",
    "            yes_probabilities.append(1 - probability)\n",
    "    \n",
    "    return yes_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc689860",
   "metadata": {},
   "source": [
    "### Get Pages and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fc47f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = extract_text_from_pdf('GSP_Drafts/1_BigValley.pdf')\n",
    "bv_pages, bv_embeddings = get_embeddings(pdf)\n",
    "\n",
    "pdf = extract_text_from_pdf('GSP_Drafts/14_ECC.pdf')\n",
    "ecc_pages, ecc_embeddings = get_embeddings(pdf)\n",
    "\n",
    "pdf = extract_text_from_pdf('GSP_Drafts/15_Fillmore.pdf')\n",
    "fillmore_pages, fillmore_embeddings = get_embeddings(pdf)\n",
    "\n",
    "pdf = extract_text_from_pdf('GSP_Drafts/30_Sonoma.pdf')\n",
    "sonoma_pages, sonoma_embeddings = get_embeddings(pdf)\n",
    "\n",
    "pdf = extract_text_from_pdf('GSP_Drafts/50_SLO.pdf')\n",
    "slo_pages, slo_embeddings = get_embeddings(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c90ded",
   "metadata": {},
   "source": [
    "### Get Rubric Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9035f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric = HumanRubric('GSP_Drafts/Rubrics/1_BigValley_DraftGSP_ScoringRubric - Coding.csv')\n",
    "bv_answers = rubric['Answer']\n",
    "bv_answers = bv_answers.drop(no_test, errors='ignore')\n",
    "bv_answers = ['No' if item == 'Somewhat' else item for item in bv_answers]\n",
    "bv_answers = ['NotApplicable' if item == 'Not Applicable' else item for item in bv_answers]\n",
    "\n",
    "rubric = HumanRubric('GSP_Drafts/Rubrics/30_SonomaValley_DraftGSP_ScoringRubric - Coding.csv')\n",
    "sonoma_answers = rubric['Answer']\n",
    "sonoma_answers = sonoma_answers.drop(no_test, errors='ignore')\n",
    "sonoma_answers = ['No' if item == 'Somewhat' else item for item in sonoma_answers]\n",
    "sonoma_answers = ['NotApplicable' if item == 'Not Applicable' else item for item in sonoma_answers]\n",
    "\n",
    "rubric = HumanRubric('GSP_Drafts/Rubrics/14_EastContraCosta_DraftGSP_ScoringRubric - Coding.csv')\n",
    "ecc_answers = rubric['Answer']\n",
    "ecc_answers = ecc_answers.drop(no_test, errors='ignore')\n",
    "ecc_answers = ['No' if item == 'Somewhat' else item for item in ecc_answers]\n",
    "ecc_answers = ['NotApplicable' if item == 'Not Applicable' else item for item in ecc_answers]\n",
    "\n",
    "rubric = HumanRubric('GSP_Drafts/Rubrics/15_Fillmore_DraftGSP_ScoringRubric - Coding.csv')\n",
    "fillmore_chat_answers = rubric['Answer']\n",
    "fillmore_chat_answers = fillmore_chat_answers.drop(no_test, errors='ignore')\n",
    "fillmore_chat_answers = ['No' if item == 'Somewhat' else item for item in fillmore_chat_answers]\n",
    "fillmore_chat_answers = ['NotApplicable' if item == 'Not Applicable' else item for item in fillmore_chat_answers]\n",
    "\n",
    "rubric = HumanRubric('GSP_Drafts/Rubrics/50_SanLuisObispoValley_DraftGSP_ScoringRubric - Coding.csv')\n",
    "slo_chat_answers = rubric['Answer']\n",
    "slo_chat_answers = slo_chat_answers.drop(no_test, errors='ignore')\n",
    "slo_chat_answers = ['No' if item == 'Somewhat' else item for item in slo_chat_answers]\n",
    "slo_chat_answers = ['NotApplicable' if item == 'Not Applicable' else item for item in slo_chat_answers]\n",
    "\n",
    "human_answers = bv_answers + sonoma_answers + ecc_answers + fillmore_chat_answers + slo_chat_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7085b74b",
   "metadata": {},
   "source": [
    "### GPT 4o Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6131ba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gde_answer(section_and_question: str) -> tuple:\n",
    "    client = OpenAI(api_key=key)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"ft:gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a skeptical environmental scientist, tasked with answering questions \"\n",
    "                    \"about a section from a Groundwater Sustainability Plan (GSP) document. You are required to provide \"\n",
    "                    \"your confidence level along with the response. Format your response as 'X, Z' where 'X' is either \"\n",
    "                    \"'Yes' or 'No' and 'Z' is your confidence level, which can be one of the following options: \"\n",
    "                    '[\"Extremely Confident, 100%\", \"Very Confident, 85%\", \"Fairly Confident, 75%\", \"Modest Confidence, 60%\", '\n",
    "                    '\"Random Guess, 50%\"]. '\n",
    "                    \"Answer the questions objectively, adhering to the provided spectrums.\"\n",
    "                    'You should only state you are \"Extremely Confident, 100%\" if it is irrefutably true that your answer is correct '\n",
    "                    \"according to the Groundwater Sustainability Plan.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": section_and_question\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    extracted = response.choices[0].message.content\n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe17118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_test = [2, 8, 9, 10, 11, 12, 13, 15, 16, 19, 20, 21, 23, 26, 27, 35, 38, 39, 69]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5552a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bv_probs = []\n",
    "for i in range(1,71):\n",
    "    if i not in no_test:\n",
    "        section = find_most_relevant_pages(bv_pages, bv_embeddings, prompts[i-1])\n",
    "        section_and_question = section + prompts[i-1]\n",
    "        bv_probs.append(gde_answer(section_and_question))\n",
    "\n",
    "ecc_probs = []\n",
    "for i in range(1,71):\n",
    "    if i not in no_test:\n",
    "        section = find_most_relevant_pages(ecc_pages, ecc_embeddings, prompts[i-1])\n",
    "        section_and_question = section + prompts[i-1]\n",
    "        ecc_probs.append(gde_answer(section_and_question))        \n",
    "        \n",
    "fillmore_probs = []\n",
    "for i in range(1,71):\n",
    "    if i not in no_test:\n",
    "        section = find_most_relevant_pages(fillmore_pages, fillmore_embeddings, prompts[i-1])\n",
    "        section_and_question = section + prompts[i-1]\n",
    "        fillmore_probs.append(gde_answer(section_and_question))\n",
    "        \n",
    "sonoma_probs = []\n",
    "for i in range(1,71):\n",
    "    if i not in no_test:\n",
    "        section = find_most_relevant_pages(sonoma_pages, sonoma_embeddings, prompts[i-1])\n",
    "        section_and_question = section + prompts[i-1]\n",
    "        sonoma_probs.append(gde_answer(section_and_question))\n",
    "        \n",
    "slo_probs = []\n",
    "for i in range(1,71):\n",
    "    if i not in no_test:\n",
    "        section = find_most_relevant_pages(slo_pages, slo_embeddings, prompts[i-1])\n",
    "        section_and_question = section + prompts[i-1]\n",
    "        slo_probs.append(gde_answer(section_and_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2802f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "fillmore_roc = extract_yes_probabilities(fillmore_probs)\n",
    "bv_roc = extract_yes_probabilities(bv_probs)\n",
    "sonoma_roc = extract_yes_probabilities(sonoma_probs)\n",
    "ecc_roc = extract_yes_probabilities(ecc_probs)\n",
    "slo_roc = extract_yes_probabilities(slo_probs)\n",
    "rocs_straight = bv_roc + sonoma_roc + ecc_roc + fillmore_roc + slo_roc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b13d240",
   "metadata": {},
   "source": [
    "### Construct Training and Validation Sets for Fine-Tuning, Output as JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9110a909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rubric_files(rubric_files, prompts, train_ratio=0.8):\n",
    "    # Function to create combined text list\n",
    "    def create_combined_list(rubric, prompts, no_test_set):\n",
    "        combined_list = []\n",
    "        for i, row in rubric.iterrows():\n",
    "            if i in no_test_set:\n",
    "                continue\n",
    "            if i < len(prompts):\n",
    "                combined_text = 'Section From GSP: ' + row['Relevant Text from GSP'] + ' Question: ' + prompts[i]\n",
    "                combined_list.append(combined_text)\n",
    "            else:\n",
    "                break\n",
    "        return combined_list\n",
    "\n",
    "    # Function to process human answers\n",
    "    def process_human_answers(human_answers, no_test_set):\n",
    "        human_answers = human_answers.drop(no_test_set, errors='ignore')\n",
    "        return ['No' if item == 'Somewhat' else item for item in human_answers]\n",
    "\n",
    "    # Function to create JSON objects\n",
    "    def create_json_objects(combined_list, human_answers, system_message):\n",
    "        json_objects = []\n",
    "        max_length = min(len(combined_list), len(human_answers))\n",
    "        for i in range(max_length):\n",
    "            json_object = {\n",
    "                \"messages\": [\n",
    "                    system_message,\n",
    "                    {\"role\": \"user\", \"content\": combined_list[i]},\n",
    "                    {\"role\": \"assistant\", \"content\": human_answers[i]}\n",
    "                ]\n",
    "            }\n",
    "            json_objects.append(json_object)\n",
    "        return json_objects\n",
    "\n",
    "    # Define the initial system message\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a skeptical environmental scientist, tasked with answering questions about a section from a Groundwater Sustainability Plan (GSP) document. You are only allowed to give one word answers. Answer the questions objectively, adhering to the provided spectrums.\"\n",
    "    }\n",
    "\n",
    "    # Initialize lists for combined data\n",
    "    all_combined_list = []\n",
    "    all_human_answers = []\n",
    "    no_test = [2, 8, 9, 10, 11, 12, 13, 15, 16, 19, 20, 21, 23, 26, 27, 35, 38, 39, 69]\n",
    "    no_test_set = set(no_test)\n",
    "\n",
    "    # Process each rubric and combine the data\n",
    "    for rubric_file in rubric_files:\n",
    "        rubric = HumanRubric(rubric_file).dropna(subset=['Relevant Text from GSP'])\n",
    "        combined_list = create_combined_list(rubric, prompts, no_test_set)\n",
    "        human_answers = process_human_answers(rubric['Answer'], no_test_set)\n",
    "        all_combined_list.extend(combined_list)\n",
    "        all_human_answers.extend(human_answers)\n",
    "\n",
    "    # Combine combined_list and human_answers into a single DataFrame for easier manipulation\n",
    "    df = pd.DataFrame({\n",
    "        'combined_text': all_combined_list,\n",
    "        'human_answer': all_human_answers\n",
    "    })\n",
    "\n",
    "    # Separate \"Yes\" and \"No\" answers\n",
    "    yes_answers = df[df['human_answer'] == 'Yes']\n",
    "    no_answers = df[df['human_answer'] == 'No']\n",
    "\n",
    "    # Shuffle both dataframes\n",
    "    yes_answers = yes_answers.sample(frac=1).reset_index(drop=True)\n",
    "    no_answers = no_answers.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Calculate number of entries for train and validation sets\n",
    "    total_entries = len(df)\n",
    "    train_size = ceil(train_ratio * total_entries)\n",
    "    val_size = total_entries - train_size\n",
    "\n",
    "    # Select equal number of \"Yes\" and \"No\" for the validation set\n",
    "    val_yes = yes_answers.iloc[:val_size//2]\n",
    "    val_no = no_answers.iloc[:val_size//2]\n",
    "\n",
    "    # Combine validation set\n",
    "    val_set = pd.concat([val_yes, val_no])\n",
    "\n",
    "    # Remaining \"Yes\" and \"No\" for the training set\n",
    "    train_yes = yes_answers.iloc[val_size//2:]\n",
    "    train_no = no_answers.iloc[val_size//2:]\n",
    "\n",
    "    # Combine training set\n",
    "    train_set = pd.concat([train_yes, train_no])\n",
    "\n",
    "    # If there are any remaining entries in df that were not selected in yes_answers and no_answers, add them to train_set\n",
    "    remaining_entries = df[~df.index.isin(train_set.index) & ~df.index.isin(val_set.index)]\n",
    "    train_set = pd.concat([train_set, remaining_entries])\n",
    "\n",
    "    # Adjust the size of the training set to ensure it contains the correct number of entries\n",
    "    train_set = train_set.sample(n=train_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Create JSON objects for training and validation data\n",
    "    train_json_objects = create_json_objects(train_set['combined_text'].tolist(), train_set['human_answer'].tolist(), system_message)\n",
    "    val_json_objects = create_json_objects(val_set['combined_text'].tolist(), val_set['human_answer'].tolist(), system_message)\n",
    "\n",
    "    # Serialize and save the training data\n",
    "    train_json_output = json.dumps(train_json_objects, indent=4)\n",
    "    with open(\"gde_train.json\", \"w\") as file:\n",
    "        file.write(train_json_output)\n",
    "\n",
    "    # Serialize and save the validation data\n",
    "    val_json_output = json.dumps(val_json_objects, indent=4)\n",
    "    with open(\"gde_val.json\", \"w\") as file:\n",
    "        file.write(val_json_output)\n",
    "\n",
    "    # Convert training data to JSONL format\n",
    "    with open(\"gde_train.jsonl\", 'w') as file:\n",
    "        for entry in train_json_objects:\n",
    "            json_string = json.dumps(entry)\n",
    "            file.write(json_string + '\\n')\n",
    "\n",
    "    # Convert validation data to JSONL format\n",
    "    with open(\"gde_val.jsonl\", 'w') as file:\n",
    "        for entry in val_json_objects:\n",
    "            json_string = json.dumps(entry)\n",
    "            file.write(json_string + '\\n')\n",
    "\n",
    "    print(\"Processing complete!\")\n",
    "    print(f\"Training set size: {train_size} entries\")\n",
    "    print(f\"Validation set size: {val_size} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bb0adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_files = [\n",
    "    'GSP_Drafts/Rubrics/11_Butte_DraftGSP_ScoringRubric - Coding.csv',\n",
    "    'GSP_Drafts/Rubrics/55_SantaMargarita_DraftGSP_ScoringRubric - Coding.csv',\n",
    "    'GSP_Drafts/Rubrics/15_Fillmore_DraftGSP_ScoringRubric - Coding.csv'\n",
    "]\n",
    "\n",
    "process_rubric_files(rubric_files, prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939f6a7f",
   "metadata": {},
   "source": [
    "### Create Fine-Tuning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac66c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_json_l(json_l):\n",
    "\n",
    "    # Set the API key\n",
    "    client = OpenAI(api_key=key)\n",
    "\n",
    "    response = client.files.create(\n",
    "      file=open(json_l, \"rb\"),\n",
    "      purpose=\"fine-tune\"\n",
    "    )\n",
    "\n",
    "    # Extract and print the file_id\n",
    "    file_id = response.id\n",
    "    print(f\"File ID: {file_id}\")\n",
    "    return file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2fbf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_id = upload_json_l('gde_train.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c571b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_file_id = upload_json_l('gde_val.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d2a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(api_key, training_file_id, validation_file_id, epochs, gpt_model, lr):\n",
    "    client = OpenAI(api_key=key)\n",
    "\n",
    "    # Create fine-tune job\n",
    "    response = client.fine_tuning.jobs.create(\n",
    "        training_file=training_file_id,\n",
    "        validation_file=validation_file_id,\n",
    "        model=gpt_model,  \n",
    "        hyperparameters={\n",
    "        \"n_epochs\":epochs,\n",
    "        \"learning_rate_multiplier\": lr   \n",
    "        }\n",
    "    )\n",
    "    model_id = response.id\n",
    "    print(f\"Model ID: {model_id}\")\n",
    "    return model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c768eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fine_tuned_model_name(job_id):\n",
    "    client = OpenAI(api_key=key)\n",
    "    \n",
    "    # Retrieve job details\n",
    "    response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "    \n",
    "    # Check if the job succeeded and retrieve the fine-tuned model name\n",
    "    if response.status == \"succeeded\":\n",
    "        fine_tuned_model = response.fine_tuned_model\n",
    "        print(f\"Fine-tuned Model Name: {fine_tuned_model}\")\n",
    "        return fine_tuned_model\n",
    "    else:\n",
    "        print(\"The job did not succeed.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a4af86",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_3_5_job = fine_tune_model(key, train_file_id, val_file_id, 8, 'gpt-3.5-turbo-0125', lr=.5)\n",
    "fine_tuned_3_5 = get_fine_tuned_model_name(fine_tuned_3_5_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa830bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_4o_job = fine_tune_model(key, train_file_id, val_file_id, 8, 'gpt-4o-2024-08-06', lr=.5)\n",
    "fine_tuned_4o = get_fine_tuned_model_name(fine_tuned_4o_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989e79c2",
   "metadata": {},
   "source": [
    "### 3.5 Fine-Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e52df6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gde_answer(section_and_question: str) -> tuple:\n",
    "    client = OpenAI(api_key=key)\n",
    "    response = client.chat.completions.create(\n",
    "        model = fine_tuned_3_5,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a skeptical environmental scientist, tasked with answering questions \"\n",
    "                    \"about a section from a Groundwater Sustainability Plan (GSP) document. You are required to provide \"\n",
    "                    \"your confidence level along with the response. Format your response as 'X, Z' where 'X' is either \"\n",
    "                    \"'Yes' or 'No' and 'Z' is your confidence level, which can be one of the following options: \"\n",
    "                    '[\"Extremely Confident, 100%\", \"Very Confident, 85%\", \"Fairly Confident, 75%\", \"Modest Confidence, 60%\", '\n",
    "                    '\"Random Guess, 50%\"]. '\n",
    "                    \"Answer the questions objectively, adhering to the provided spectrums.\"\n",
    "                    'You should only state you are \"Extremely Confident, 100%\" if it is irrefutably true that your answer is correct '\n",
    "                    \"according to the Groundwater Sustainability Plan.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": section_and_question\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    extracted = response.choices[0].message.content\n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e92b7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bv_probs_3_5 = []\n",
    "for i in range(1,71):\n",
    "    if i not in no_test:\n",
    "        section = find_most_relevant_pages(bv_pages, bv_embeddings, prompts[i-1])\n",
    "        section_and_question = section + prompts[i-1]\n",
    "        bv_probs_3_5.append(gde_answer(section_and_question))\n",
    "\n",
    "ecc_probs_3_5 = []\n",
    "for i in range(1,71):\n",
    "    if i not in no_test:\n",
    "        section = find_most_relevant_pages(ecc_pages, ecc_embeddings, prompts[i-1])\n",
    "        section_and_question = section + prompts[i-1]\n",
    "        ecc_probs_3_5.append(gde_answer(section_and_question))        \n",
    "        \n",
    "fillmore_probs_3_5 = []\n",
    "for i in range(1,71):\n",
    "    if i not in no_test:\n",
    "        section = find_most_relevant_pages(fillmore_pages, fillmore_embeddings, prompts[i-1])\n",
    "        section_and_question = section + prompts[i-1]\n",
    "        fillmore_probs_3_5.append(gde_answer(section_and_question))\n",
    "        \n",
    "sonoma_probs_3_5 = []\n",
    "for i in range(1,71):\n",
    "    if i not in no_test:\n",
    "        section = find_most_relevant_pages(sonoma_pages, sonoma_embeddings, prompts[i-1])\n",
    "        section_and_question = section + prompts[i-1]\n",
    "        sonoma_probs_3_5.append(gde_answer(section_and_question))\n",
    "        \n",
    "slo_probs_3_5 = []\n",
    "for i in range(1,71):\n",
    "    if i not in no_test:\n",
    "        section = find_most_relevant_pages(slo_pages, slo_embeddings, prompts[i-1])\n",
    "        section_and_question = section + prompts[i-1]\n",
    "        slo_probs_3_5.append(gde_answer(section_and_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15147e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "fillmore_roc_3_5 = extract_yes_probabilities(fillmore_probs_3_5)\n",
    "bv_roc_3_5 = extract_yes_probabilities(bv_probs_3_5)\n",
    "sonoma_roc_3_5 = extract_yes_probabilities(sonoma_probs_3_5)\n",
    "ecc_roc_3_5 = extract_yes_probabilities(ecc_probs_3_5)\n",
    "slo_roc_3_5 = extract_yes_probabilities(slo_probs_3_5)\n",
    "rocs_3_5 = bv_roc_3_5 + sonoma_roc_3_5 + ecc_roc_3_5 + fillmore_roc_3_5 + slo_roc_3_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b764d8",
   "metadata": {},
   "source": [
    "### 4o Fine-Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2c536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gde_answer(section_and_question: str) -> tuple:\n",
    "    client = OpenAI(api_key=key)\n",
    "    response = client.chat.completions.create(\n",
    "        model= fine_tuned_4o,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a skeptical environmental scientist, tasked with answering questions \"\n",
    "                    \"about a section from a Groundwater Sustainability Plan (GSP) document. You are required to provide \"\n",
    "                    \"your confidence level along with the response. Format your response as 'X, Z' where 'X' is either \"\n",
    "                    \"'Yes' or 'No' and 'Z' is your confidence level, which can be one of the following options: \"\n",
    "                    '[\"Extremely Confident, 100%\", \"Very Confident, 85%\", \"Fairly Confident, 75%\", \"Modest Confidence, 60%\", '\n",
    "                    '\"Random Guess, 50%\"]. '\n",
    "                    \"Answer the questions objectively, adhering to the provided spectrums.\"\n",
    "                    'You should only state you are \"Extremely Confident, 100%\" if it is irrefutably true that your answer is correct '\n",
    "                    \"according to the Groundwater Sustainability Plan.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": section_and_question\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    extracted = response.choices[0].message.content\n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d22962",
   "metadata": {},
   "outputs": [],
   "source": [
    "bv_probs_4o = []\n",
    "for i in range(1,71):\n",
    "    if i not in no_test:\n",
    "        section = find_most_relevant_pages(bv_pages, bv_embeddings, prompts[i-1])\n",
    "        section_and_question = section + prompts[i-1]\n",
    "        bv_probs_4o.append(gde_answer(section_and_question))\n",
    "\n",
    "ecc_probs_4o = []\n",
    "for i in range(1,71):\n",
    "    if i not in no_test:\n",
    "        section = find_most_relevant_pages(ecc_pages, ecc_embeddings, prompts[i-1])\n",
    "        section_and_question = section + prompts[i-1]\n",
    "        ecc_probs_4o.append(gde_answer(section_and_question))        \n",
    "        \n",
    "fillmore_probs_4o = []\n",
    "for i in range(1,71):\n",
    "    if i not in no_test:\n",
    "        section = find_most_relevant_pages(fillmore_pages, fillmore_embeddings, prompts[i-1])\n",
    "        section_and_question = section + prompts[i-1]\n",
    "        fillmore_probs_4o.append(gde_answer(section_and_question))\n",
    "        \n",
    "sonoma_probs_4o = []\n",
    "for i in range(1,71):\n",
    "    if i not in no_test:\n",
    "        section = find_most_relevant_pages(sonoma_pages, sonoma_embeddings, prompts[i-1])\n",
    "        section_and_question = section + prompts[i-1]\n",
    "        sonoma_probs_4o.append(gde_answer(section_and_question))\n",
    "        \n",
    "slo_probs_4o = []\n",
    "for i in range(1,71):\n",
    "    if i not in no_test:\n",
    "        section = find_most_relevant_pages(slo_pages, slo_embeddings, prompts[i-1])\n",
    "        section_and_question = section + prompts[i-1]\n",
    "        slo_probs_4o.append(gde_answer(section_and_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82cc372",
   "metadata": {},
   "outputs": [],
   "source": [
    "fillmore_roc_4o = extract_yes_probabilities(fillmore_probs_4o)\n",
    "bv_roc_4o = extract_yes_probabilities(bv_probs_4o)\n",
    "sonoma_roc_4o = extract_yes_probabilities(sonoma_probs_4o)\n",
    "ecc_roc_4o = extract_yes_probabilities(ecc_probs_4o)\n",
    "slo_roc_4o = extract_yes_probabilities(slo_probs_4o)\n",
    "rocs_4o = bv_roc_4o + sonoma_roc_4o + ecc_roc_4o + fillmore_roc_4o + slo_roc_4o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a105fc",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9944a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Human Answers', 'Rocs_Straight', 'Rocs_3.5', 'Rocs_4o']\n",
    "\n",
    "# Create a dictionary with the lists as values\n",
    "data = {\n",
    "    'Human Answers': human_answers,\n",
    "    'Rocs_Straight': rocs_straight,\n",
    "    'Rocs_3.5': rocs_3_5,\n",
    "    'Rocs_4o': rocs_4o\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "labels = ['BigValley'] * 51 + ['Sonoma'] * 51 + ['East Contra Costa'] * 51 + ['Fillmore'] * 51 + ['San Luis Obispo'] * 51\n",
    "\n",
    "# Assign this list as a new column 'GSP' in the DataFrame\n",
    "df['GSP'] = labels\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('graphs9_29.csv', index=False)\n",
    "\n",
    "print(\"CSV saved as graphs9_29.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
